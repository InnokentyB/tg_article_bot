"""
BART-based zero-shot classification for article categorization
Alternative categorization approach using transformers
"""
import logging
from typing import List, Dict, Tuple
import re

try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

# Rule-based categorization as fallback
CATEGORY_RULES = {
    "–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ / –ò–Ω–∂–µ–Ω–µ—Ä–∏—è": {
        "keywords": ["—Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑", "—Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫", "uml", "bpmn", "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ", 
                    "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "–∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π", "–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º"],
        "weight": 1.0
    },
    "HR / –†—ã–Ω–æ–∫ —Ç—Ä—É–¥–∞ IT": {
        "keywords": ["–≤–∞–∫–∞–Ω—Å–∏–∏", "—Ä–∞–±–æ—Ç–∞", "–∫–∞—Ä—å–µ—Ä–∞", "—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ", "—Ä—ã–Ω–æ–∫ —Ç—Ä—É–¥–∞", "–∑–∞—Ä–ø–ª–∞—Ç—ã", 
                    "hr", "—Ä–µ–∫—Ä—É—Ç–∏–Ω–≥", "—Ä–µ–∑—é–º–µ", "–ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç—ã", "—Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"],
        "weight": 1.0
    },
    "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": {
        "keywords": ["python", "javascript", "react", "node.js", "frameworks", "–±–∏–±–ª–∏–æ—Ç–µ–∫–∏", 
                    "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "ide", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"],
        "weight": 0.9
    },
    "–ó–∞—Ä–ø–ª–∞—Ç—ã –∏ –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏": {
        "keywords": ["–∑–∞—Ä–ø–ª–∞—Ç–∞", "–æ–∫–ª–∞–¥", "–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏", "–±–æ–Ω—É—Å—ã", "–¥–æ—Ö–æ–¥—ã", "–∑–∞—Ä–∞–±–æ—Ç–æ–∫", 
                    "—Å—Ç–æ–∏–º–æ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞", "–∑–∞—Ä–ø–ª–∞—Ç–Ω–∞—è –≤–∏–ª–∫–∞"],
        "weight": 0.9
    },
    "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏": {
        "keywords": ["–Ω–∞–≤—ã–∫–∏", "–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏", "–æ–±—É—á–µ–Ω–∏–µ", "–∫—É—Ä—Å—ã", "—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è", 
                    "–ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏", "—Ä–∞–∑–≤–∏—Ç–∏–µ", "—Å–∫–∏–ª–ª—ã"],
        "weight": 0.8
    },
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ": {
        "keywords": ["–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", "–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ø–∞—Ç—Ç–µ—Ä–Ω—ã", "ddd", "–º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å—ã", 
                    "design patterns", "—Å–∏—Å—Ç–µ–º–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞"],
        "weight": 0.8
    },
    "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏": {
        "keywords": ["—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏", "–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç", "agile", "scrum", "kanban", 
                    "–ø—Ä–æ–µ–∫—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"],
        "weight": 0.8
    },
    "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏": {
        "keywords": ["–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "security", "–∑–∞—â–∏—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "–∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—è", 
                    "–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "—É—è–∑–≤–∏–º–æ—Å—Ç–∏"],
        "weight": 0.8
    },
    "–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ": {
        "keywords": ["–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "–æ–±—É—á–µ–Ω–∏–µ", "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–∫—É—Ä—Å—ã", "–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã", 
                    "—Å—Ç—É–¥–µ–Ω—Ç—ã", "–ø—Ä–µ–ø–æ–¥–∞–≤–∞–Ω–∏–µ"],
        "weight": 0.7
    },
    "–ë–∏–∑–Ω–µ—Å –∏ —Ñ–∏–Ω–∞–Ω—Å—ã": {
        "keywords": ["–±–∏–∑–Ω–µ—Å", "—Ñ–∏–Ω–∞–Ω—Å—ã", "—Å—Ç–∞—Ä—Ç–∞–ø", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–Ω–∞–ª–æ–≥–∏", "–Ω–∞–ª–æ–≥–æ–æ–±–ª–æ–∂–µ–Ω–∏–µ", 
                    "—ç–º–∏–≥—Ä–∞—Ü–∏—è", "—Ä–µ–∑–∏–¥–µ–Ω—Ç—Å—Ç–≤–æ", "—Ñ–Ω—Å", "–Ω–¥—Ñ–ª", "–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å—Å—Ç–≤–æ"],
        "weight": 0.8
    }
}

logger = logging.getLogger(__name__)

class BartCategorizer:
    """
    Zero-shot classification using BART model
    Provides categorical classification with confidence scores
    """
    
    def __init__(self):
        """Initialize BART categorizer"""
        self.classifier = None
        self.candidate_labels = [
            "–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ / –ò–Ω–∂–µ–Ω–µ—Ä–∏—è",
            "HR / –†—ã–Ω–æ–∫ —Ç—Ä—É–¥–∞ IT", 
            "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
            "–ó–∞—Ä–ø–ª–∞—Ç—ã –∏ –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏",
            "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏",
            "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏",
            "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
            "–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ",
            "–ë–∏–∑–Ω–µ—Å –∏ —Ñ–∏–Ω–∞–Ω—Å—ã"
        ]
        
        if TRANSFORMERS_AVAILABLE:
            try:
                logger.info("Initializing BART classifier...")
                self.classifier = pipeline(
                    "zero-shot-classification", 
                    model="facebook/bart-large-mnli"
                )
                logger.info("BART classifier initialized successfully")
            except Exception as e:
                logger.error(f"Failed to initialize BART classifier: {e}")
                self.classifier = None
        else:
            logger.warning("Transformers not available, BART classifier disabled")
    
    def is_available(self) -> bool:
        """Check if BART classifier is available"""
        return self.classifier is not None
    
    def _clean_text(self, text: str) -> str:
        """Clean text for processing"""
        if not text:
            return ""
        
        # Remove URLs, special characters, and normalize whitespace
        text = re.sub(r'https?://\S+', '', text)
        text = re.sub(r'[^\w\s\-.,!?;:]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def categorize_article(self, text: str, title: str = "") -> Dict:
        """
        Categorize article using BART zero-shot classification or rule-based fallback
        
        Args:
            text: Article text
            title: Article title (optional)
            
        Returns:
            Dict with classification results
        """
        try:
            # Prepare text
            full_text = f"{title} {text}" if title else text
            clean_text = self._clean_text(full_text)
            
            if len(clean_text) < 50:
                return {
                    "primary_category": "–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç",
                    "categories": [],
                    "confidence": 0.0,
                    "method": "text_too_short"
                }
            
            # Try BART if available
            if self.is_available():
                # Limit text length for BART (max ~1000 tokens)
                if len(clean_text) > 4000:
                    clean_text = clean_text[:4000] + "..."
                
                # Perform zero-shot classification
                result = self.classifier(clean_text, self.candidate_labels, multi_label=True)
                
                # Extract results
                categories_with_scores = list(zip(result['labels'], result['scores']))
                
                # Get primary category (highest confidence)
                primary_category = categories_with_scores[0][0] if categories_with_scores else "Unknown"
                primary_confidence = categories_with_scores[0][1] if categories_with_scores else 0.0
                
                # Get top categories with confidence > 0.1
                top_categories = [
                    {"category": label, "confidence": score} 
                    for label, score in categories_with_scores 
                    if score > 0.1
                ][:3]  # Top 3
                
                return {
                    "primary_category": primary_category,
                    "categories": top_categories,
                    "confidence": primary_confidence,
                    "method": "bart_zero_shot",
                    "all_scores": categories_with_scores
                }
            else:
                # Use rule-based classification as fallback
                return self._rule_based_classification(clean_text)
            
        except Exception as e:
            logger.error(f"Error in BART categorization: {e}")
            # Fallback to rule-based on error
            try:
                return self._rule_based_classification(self._clean_text(f"{title} {text}"))
            except:
                return {
                    "primary_category": "–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏",
                    "categories": [],
                    "confidence": 0.0,
                    "method": "error",
                    "error": str(e)
                }
    
    def _rule_based_classification(self, text: str) -> Dict:
        """Rule-based classification as fallback when BART is unavailable"""
        logger.info("   üîß –ó–∞–ø—É—Å–∫ –ø—Ä–∞–≤–∏–ª–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
        text_lower = text.lower()
        
        category_scores = {}
        
        # Calculate scores for each category
        logger.info("   üìä –ê–Ω–∞–ª–∏–∑ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏...")
        for category, data in CATEGORY_RULES.items():
            score = 0
            matched_keywords = []
            
            for keyword in data["keywords"]:
                if keyword.lower() in text_lower:
                    score += 1
                    matched_keywords.append(keyword)
            
            if score > 0:
                # Normalize score
                normalized_score = min((score / len(data["keywords"])) * data["weight"], 0.95)
                category_scores[category] = {
                    "score": normalized_score,
                    "matched_keywords": matched_keywords
                }
        
        if not category_scores:
            logger.info("   ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º")
            return {
                "primary_category": "–û–±—â–∞—è —Ç–µ–º–∞",
                "categories": [],
                "confidence": 0.1,
                "method": "rule_based_fallback"
            }
        
        # Sort by score
        logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(category_scores)} –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        sorted_categories = sorted(category_scores.items(), key=lambda x: x[1]["score"], reverse=True)
        
        for i, (cat, data) in enumerate(sorted_categories[:3]):
            logger.info(f"   {i+1}. {cat}: {data['score']:.2%} (—Ç–µ—Ä–º–∏–Ω—ã: {data['matched_keywords'][:3]})")
        
        primary_category = sorted_categories[0][0]
        primary_confidence = sorted_categories[0][1]["score"]
        
        # Build top categories
        top_categories = [
            {
                "category": cat,
                "confidence": data["score"],
                "matched_keywords": data["matched_keywords"]
            }
            for cat, data in sorted_categories[:3]
            if data["score"] > 0.05
        ]
        
        return {
            "primary_category": primary_category,
            "categories": top_categories,
            "confidence": primary_confidence,
            "method": "rule_based_classification",
            "matched_keywords": sorted_categories[0][1]["matched_keywords"]
        }
    
    def get_category_mapping(self) -> Dict[str, str]:
        """Get mapping of BART categories to system categories"""
        return {
            "–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ / –ò–Ω–∂–µ–Ω–µ—Ä–∏—è": "Architecture",
            "HR / –†—ã–Ω–æ–∫ —Ç—Ä—É–¥–∞ IT": "Career", 
            "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": "Programming",
            "–ó–∞—Ä–ø–ª–∞—Ç—ã –∏ –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏": "Career",
            "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏": "Career",
            "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ": "Architecture",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏": "Management",
            "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏": "Security",
            "–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ": "Education",
            "–ë–∏–∑–Ω–µ—Å –∏ —Ñ–∏–Ω–∞–Ω—Å—ã": "Business"
        }