"""
Advanced Article Categorizer using UpSkillMe service
Integrates OpenAI-based categorization with embedding similarity
"""
import os
import re
import uuid
import yaml
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional
from openai import OpenAI
from topic_clusterer import TopicClusterer
from bart_categorizer import BartCategorizer

logger = logging.getLogger(__name__)

class AdvancedCategorizer:
    """Advanced categorizer using LLM + embeddings for precise categorization"""
    
    def __init__(self):
        # Only initialize OpenAI client if API key is available
        if os.getenv("OPENAI_API_KEY"):
            self.client = OpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_BASE_URL", None)
            )
        else:
            self.client = None
            logger.warning("OpenAI API key not found, advanced categorization will be disabled")
        
        self.embed_model = os.getenv("OPENAI_EMBED_MODEL", "text-embedding-3-small")
        self.chat_model = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini")
        
        # Load categories configuration
        self.categories = self._load_categories()
        self._primary_embeddings = None
        
        # Initialize topic clusterer
        try:
            self.topic_clusterer = TopicClusterer()
            logger.info("Topic clusterer initialized")
        except Exception as e:
            logger.error(f"Failed to initialize topic clusterer: {e}")
            self.topic_clusterer = None
        
        # Initialize BART categorizer
        try:
            self.bart_categorizer = BartCategorizer()
            logger.info("BART categorizer initialized")
        except Exception as e:
            logger.error(f"Failed to initialize BART categorizer: {e}")
            self.bart_categorizer = None
        
        logger.info("Advanced categorizer initialized")
    
    def _load_categories(self) -> Dict:
        """Load categories from YAML config"""
        categories_path = os.path.join(os.path.dirname(__file__), "categories.yaml")
        try:
            with open(categories_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            logger.error("categories.yaml not found, using default categories")
            return {
                "primary": [
                    {"key": "AI", "label": "Ð˜ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚", "description": "ML, Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚Ð¸, LLM"},
                    {"key": "Programming", "label": "ÐŸÑ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ", "description": "Ð¯Ð·Ñ‹ÐºÐ¸, Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¸, Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ñ‹"},
                    {"key": "Business", "label": "Ð‘Ð¸Ð·Ð½ÐµÑ", "description": "Ð‘Ð¸Ð·Ð½ÐµÑ-Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹, Ð¼ÐµÐ½ÐµÐ´Ð¶Ð¼ÐµÐ½Ñ‚"},
                    {"key": "Other", "label": "Ð”Ñ€ÑƒÐ³Ð¾Ðµ", "description": "ÐŸÑ€Ð¾Ñ‡Ð¸Ðµ Ñ‚ÐµÐ¼Ñ‹"}
                ],
                "subcategories": {
                    "AI": ["LLM", "NLP", "Computer Vision"],
                    "Programming": ["Python", "JavaScript", "Testing"],
                    "Business": ["Management", "Strategy"],
                    "Other": ["General"]
                }
            }
    
    def _clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        if not text:
            return ""
        # Remove HTML tags
        text = re.sub(r"<[^>]+>", " ", text)
        # Normalize whitespace
        text = re.sub(r"\s+", " ", text).strip()
        return text
    
    def _summarize(self, text: str, title: str = "") -> str:
        """Generate summary using LLM"""
        try:
            content = f"{title}\n\n{text}" if title else text
            content = content[:6000]  # Limit content size
            
            prompt = f"""Ð¡ÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ñ‚ÐµÐºÑÑ‚ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð¸/ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð² 3-5 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸ÑÑ… Ð½Ð° ÑÐ·Ñ‹ÐºÐµ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°. 
Ð’Ñ‹Ð´ÐµÐ»Ð¸ Ð¾ÑÐ½Ð¾Ð²Ð½ÑƒÑŽ Ñ‚ÐµÐ¼Ñƒ Ð¸ ÐºÐ»ÑŽÑ‡ÐµÐ²ÑƒÑŽ Ð¼Ñ‹ÑÐ»ÑŒ.

Ð¢ÐµÐºÑÑ‚: ```{content}```"""
            
            response = self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "system", "content": "Ð¢Ñ‹ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¸ Ñ‚Ð¾Ñ‡Ð½Ð¾ ÑÑƒÐ¼Ð¼Ð¸Ñ€ÑƒÐµÑˆÑŒ Ñ‚ÐµÐºÑÑ‚Ñ‹."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=220
            )
            
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Error generating summary: {e}")
            return text[:500] + "..." if len(text) > 500 else text
    
    def _embed_texts(self, texts: List[str]) -> np.ndarray:
        """Generate embeddings for texts"""
        try:
            response = self.client.embeddings.create(
                model=self.embed_model, 
                input=texts
            )
            vectors = [np.array(d.embedding, dtype=np.float32) for d in response.data]
            return np.stack(vectors, axis=0)
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            # Return random embeddings as fallback
            return np.random.randn(len(texts), 1536).astype(np.float32)
    
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """Calculate cosine similarity between vectors"""
        a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-9)
        b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-9)
        return np.dot(a_norm, b_norm.T)
    
    def _get_primary_embeddings(self) -> np.ndarray:
        """Get cached primary category embeddings"""
        if self._primary_embeddings is None:
            descriptions = [cat["description"] for cat in self.categories["primary"]]
            self._primary_embeddings = self._embed_texts(descriptions)
        return self._primary_embeddings
    
    def _classify_primary(self, summary: str) -> Tuple[str, float]:
        """Classify primary category using embeddings with improved logic"""
        try:
            query_embedding = self._embed_texts([summary])
            category_embeddings = self._get_primary_embeddings()
            
            similarities = self._cosine_similarity(query_embedding, category_embeddings)[0]
            
            # Get top 3 categories for better analysis
            top_indices = np.argsort(similarities)[-3:][::-1]
            top_similarities = similarities[top_indices]
            
            # Choose best category with improved confidence calculation
            best_idx = top_indices[0]
            raw_confidence = float(top_similarities[0])
            
            # Boost confidence if similarity is strong or multiple categories are close
            if raw_confidence > 0.4 or (len(top_similarities) > 1 and top_similarities[1] > 0.3):
                confidence = min(raw_confidence * 1.2, 0.95)  # Boost but cap at 95%
            else:
                confidence = max(raw_confidence, 0.25)  # Minimum 25%
            
            primary_key = self.categories["primary"][best_idx]["key"]
            return primary_key, confidence
            
        except Exception as e:
            logger.error(f"Error in primary classification: {e}")
            # Enhanced fallback with keyword-based confidence
            fallback_category, confidence = self._fallback_classification(summary)
            return fallback_category, confidence
    
    def _extract_subcategories(self, primary_key: str, summary: str, max_categories: int = 3) -> List[str]:
        """Extract subcategories using keyword matching + LLM fallback"""
        try:
            subcategories = self.categories["subcategories"].get(primary_key, [])
            if not subcategories:
                return []
            
            # First try keyword-based matching for Business category
            if primary_key == "Business":
                summary_lower = summary.lower()
                
                # Tax-related keywords
                if any(word in summary_lower for word in ["Ð½Ð°Ð»Ð¾Ð³", "Ð½Ð´Ñ„Ð»", "Ð½Ð°Ð»Ð¾Ð³Ð¾Ð¾Ð±Ð»Ð¾Ð¶ÐµÐ½", "tax"]):
                    if "Taxes" in subcategories:
                        return ["Taxes"]
                
                # Legal keywords  
                if any(word in summary_lower for word in ["Ð¿Ñ€Ð°Ð²Ð¾Ð²", "Ð·Ð°ÐºÐ¾Ð½", "ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐº", "legal"]):
                    if "Legal" in subcategories:
                        return ["Legal"]
                
                # Immigration keywords
                if any(word in summary_lower for word in ["ÑÐ¼Ð¸Ð³Ñ€Ð°Ñ†", "Ð¸Ð¼Ð¼Ð¸Ð³Ñ€Ð°Ñ†", "Ð¿ÐµÑ€ÐµÐµÐ·Ð´", "immigration"]):
                    if "Immigration" in subcategories:
                        return ["Immigration"]
                        
                # Investment keywords
                if any(word in summary_lower for word in ["Ð¸Ð½Ð²ÐµÑÑ‚Ð¸Ñ†", "investment", "Ð¸Ð½Ð²ÐµÑÑ‚Ð¾Ñ€"]):
                    if "Investment" in subcategories:
                        return ["Investment"]
            
            # Career category keyword matching
            if primary_key == "Career":
                summary_lower = summary.lower()
                
                # Job market and positions
                if any(word in summary_lower for word in ["Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸", "Ð´Ð¾Ð»Ð¶Ð½Ð¾ÑÑ‚", "job", "position", "Ñ€Ñ‹Ð½Ð¾Ðº Ñ‚Ñ€ÑƒÐ´Ð°"]):
                    if "Industry Trends" in subcategories:
                        return ["Industry Trends"]
                
                # Interview and skills
                if any(word in summary_lower for word in ["ÑÐ¾Ð±ÐµÑÐµÐ´Ð¾Ð²Ð°Ð½", "interview", "Ð½Ð°Ð²Ñ‹ÐºÐ¸", "skill"]):
                    if "Interview Prep" in subcategories:
                        return ["Interview Prep"]
            
            # Fallback to LLM for other categories or unclear cases
            prompt = f"""ÐÐ° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ð³Ð¾ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð²Ñ‹Ð±ÐµÑ€Ð¸ Ð´Ð¾ {max_categories} Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ñ… Ð¿Ð¾Ð´ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¹ Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ°.
Ð”Ð°Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð² Ð²Ð¸Ð´Ðµ JSON Ð¼Ð°ÑÑÐ¸Ð²Ð° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ð¼Ð¸ ÑÑ‚Ñ€Ð¾ÐºÐ°Ð¼Ð¸, Ð±ÐµÐ· ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ².

Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð¿Ð¾Ð´ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸: {subcategories}

ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ ÑÑ‚Ð°Ñ‚ÑŒÐ¸: {summary}"""
            
            response = self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "system", "content": "Ð¢Ñ‹ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐµÑˆÑŒ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¿Ð¾ Ð¿Ð¾Ð´ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑÐ¼. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ JSON Ð¼Ð°ÑÑÐ¸Ð²Ð°."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=100
            )
            
            result_text = response.choices[0].message.content.strip()
            # Try to parse as JSON
            import json
            try:
                result = json.loads(result_text)
                if isinstance(result, list):
                    return [sub for sub in result if sub in subcategories][:max_categories]
            except json.JSONDecodeError:
                pass
            
            # Fallback: return first subcategory
            return [subcategories[0]] if subcategories else []
            
        except Exception as e:
            logger.error(f"Error extracting subcategories: {e}")
            return []
    
    def _extract_keywords(self, summary: str, max_keywords: int = 8) -> List[str]:
        """Extract keywords using LLM"""
        try:
            prompt = f"""Ð˜Ð·Ð²Ð»ÐµÐºÐ¸ Ð´Ð¾ {max_keywords} ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ»Ð¾Ð² Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÐ¸.
Ð”Ð°Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð² Ð²Ð¸Ð´Ðµ JSON Ð¼Ð°ÑÑÐ¸Ð²Ð° ÑÑ‚Ñ€Ð¾Ðº Ð±ÐµÐ· ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ².

Ð¢ÐµÐºÑÑ‚: {summary}"""
            
            response = self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "system", "content": "Ð¢Ñ‹ Ð¸Ð·Ð²Ð»ÐµÐºÐ°ÐµÑˆÑŒ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð° Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð¾Ð². ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ JSON Ð¼Ð°ÑÑÐ¸Ð²Ð°."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=150
            )
            
            result_text = response.choices[0].message.content.strip()
            # Try to parse as JSON
            import json
            try:
                result = json.loads(result_text)
                if isinstance(result, list):
                    return [str(kw) for kw in result[:max_keywords]]
            except json.JSONDecodeError:
                pass
            
            # Fallback: simple keyword extraction
            words = summary.split()
            return [w for w in words if len(w) > 4][:max_keywords]
            
        except Exception as e:
            logger.error(f"Error extracting keywords: {e}")
            return []
    
    async def categorize_article(self, text: str, title: str = "", language: str = "auto", extracted_keywords: list = None) -> Dict:
        """
        Main categorization method
        
        Args:
            text: Article text
            title: Article title (optional)
            language: Language hint (auto/ru/en)
            
        Returns:
            Dict with categorization results
        """
        try:
            # Clean input text
            clean_text = self._clean_text(text)
            clean_title = self._clean_text(title)
            
            if not clean_text and not clean_title:
                raise ValueError("No text content provided")
            
            # Generate summary
            logger.info("=== ÐÐÐ§ÐÐ›Ðž Ð”Ð•Ð¢ÐÐ›Ð¬ÐÐžÐ“Ðž ÐÐÐÐ›Ð˜Ð—Ð Ð¡Ð¢ÐÐ¢Ð¬Ð˜ ===")
            logger.info(f"ðŸ“ Ð¢ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°: {len(clean_text)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
            logger.info(f"ðŸ“° Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº: '{clean_title}'")
            
            logger.info("ðŸ”„ Ð­Ð¢ÐÐŸ 1: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ð³Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ñ...")
            summary = self._summarize(clean_text, clean_title)
            logger.info(f"âœ… ÐšÑ€Ð°Ñ‚ÐºÐ¾Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ ÑÐ¾Ð·Ð´Ð°Ð½Ð¾ ({len(summary)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²)")
            logger.info(f"ðŸ“‹ Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ: {summary[:200]}...")
            
            # Primary category classification
            logger.info("ðŸ”„ Ð­Ð¢ÐÐŸ 2: AI ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸...")
            primary_key, confidence = self._classify_primary(summary)
            logger.info(f"âœ… AI ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ: {primary_key} (ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ: {confidence:.2%})")
            
            # Get primary category label
            primary_label = next(
                (cat["label"] for cat in self.categories["primary"] if cat["key"] == primary_key),
                primary_key
            )
            
            # Extract subcategories
            logger.info("ðŸ”„ Ð­Ð¢ÐÐŸ 3a: Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð´ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¹...")
            subcategories = self._extract_subcategories(primary_key, summary)
            logger.info(f"âœ… ÐŸÐ¾Ð´ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸: {subcategories}")
            
            # Use extracted keywords from HTML if available, otherwise extract from text
            logger.info("ðŸ”„ Ð­Ð¢ÐÐŸ 3b: Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ»Ð¾Ð²...")
            if extracted_keywords:
                logger.info(f"âœ… Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð° Ð¸Ð· HTML: {extracted_keywords}")
                keywords = extracted_keywords[:8]  # Limit to 8 keywords
            else:
                logger.info("ðŸ”„ Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð° Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°...")
                keywords = self._extract_keywords(summary)
                logger.info(f"âœ… Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð½Ñ‹Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð°: {keywords}")
            
            # Topic clustering (semantic analysis)
            logger.info("ðŸ”„ Ð­Ð¢ÐÐŸ 4: Ð¢ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ (TF-IDF)...")
            topic_result = None
            if self.topic_clusterer:
                try:
                    topic_result = self.topic_clusterer.cluster_document(clean_text, clean_title)
                    if topic_result:
                        logger.info(f"âœ… Ð¢ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ:")
                        logger.info(f"   ðŸ“Š Ð¢ÐµÐ¼Ð°: {topic_result.get('topic_label', 'ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾')}")
                        logger.info(f"   ðŸ·ï¸ ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ñ‹: {topic_result.get('topic_keywords', [])}")
                        logger.info(f"   ðŸ“ˆ Ð£Ð²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ: {topic_result.get('confidence', 0):.1%}")
                    else:
                        logger.warning("âš ï¸ Ð¢ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½Ðµ Ð´Ð°Ð»Ð° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°")
                except Exception as e:
                    logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {e}")
            else:
                logger.warning("âš ï¸ Ð¢ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°")
            
            # BART categorization (zero-shot classification)
            logger.info("ðŸ”„ Ð­Ð¢ÐÐŸ 5: BART/Rule-based ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ...")
            bart_result = None
            if self.bart_categorizer:
                try:
                    bart_result = self.bart_categorizer.categorize_article(clean_text, clean_title)
                    if bart_result:
                        method = bart_result.get('method', 'unknown')
                        logger.info(f"âœ… Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ ({method}):")
                        logger.info(f"   ðŸŽ¯ ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ: {bart_result.get('primary_category', 'ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾')}")
                        logger.info(f"   ðŸŽ² Ð£Ð²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ: {bart_result.get('confidence', 0):.1%}")
                        if bart_result.get('matched_keywords'):
                            logger.info(f"   ðŸ”‘ ÐÐ°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ðµ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ñ‹: {bart_result['matched_keywords']}")
                    else:
                        logger.warning("âš ï¸ Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð½Ðµ Ð´Ð°Ð»Ð° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°")
                except Exception as e:
                    logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸: {e}")
            else:
                logger.warning("âš ï¸ Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°")
            
            result = {
                "id": str(uuid.uuid4()),
                "title": clean_title or None,
                "summary": summary,
                # AI-based structured categorization
                "ai_categorization": {
                    "primary_category": primary_key,
                    "primary_category_label": primary_label,
                    "subcategories": subcategories,
                    "keywords": keywords,
                    "confidence": confidence,
                    "method": "openai_embeddings"
                },
                # Topic clustering categorization
                "topic_clustering": topic_result,
                # BART zero-shot categorization
                "bart_categorization": bart_result,
                # Legacy fields for backward compatibility
                "primary_category": primary_key,
                "primary_category_label": primary_label,
                "subcategories": subcategories,
                "keywords": keywords,
                "confidence": confidence
            }
            
            logger.info("=== Ð—ÐÐ’Ð•Ð Ð¨Ð•ÐÐ˜Ð• ÐÐÐÐ›Ð˜Ð—Ð ===")
            logger.info(f"ðŸŽ‰ Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ: {primary_key} (ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ: {confidence:.2f})")
            logger.info("ðŸ“Š Ð’ÑÐµ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ñ‹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ñ‹ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾")
            logger.info("=" * 50)
            return result
            
        except Exception as e:
            logger.error(f"Categorization failed: {e}")
            raise
    
    def _fallback_classification(self, text: str) -> Tuple[str, float]:
        """Enhanced fallback classification based on keywords"""
        text_lower = text.lower()
        
        # Category keyword mapping with weights
        category_keywords = {
            "AI": {
                "keywords": ["ai", "Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ", "Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚Ð¸", "llm", "nlp", "Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸", 
                           "Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚", "computer vision", "mlops"],
                "weight": 0.9
            },
            "Programming": {
                "keywords": ["Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ", "Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°", "ÐºÐ¾Ð´", "Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼", "python", "javascript", 
                           "Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¸", "software", "development", "coding", "Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¸"],
                "weight": 0.8
            },
            "Business": {
                "keywords": ["Ð±Ð¸Ð·Ð½ÐµÑ", "Ð´ÐµÐ½ÑŒÐ³Ð¸", "Ñ„Ð¸Ð½Ð°Ð½ÑÑ‹", "ÑÑ‚Ð°Ñ€Ñ‚Ð°Ð¿", "Ð¼ÐµÐ½ÐµÐ´Ð¶Ð¼ÐµÐ½Ñ‚", "Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¸Ð½Ð³", 
                           "business", "finance", "startup", "investment", "Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ð¸", "Ð½Ð°Ð»Ð¾Ð³Ð¸", "Ð½Ð°Ð»Ð¾Ð³Ð¾Ð¾Ð±Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ", 
                           "Ñ„Ð½Ñ", "Ð½Ð°Ð»Ð¾Ð³Ð¾Ð²Ð¾Ðµ", "Ñ€ÐµÐ·Ð¸Ð´ÐµÐ½Ñ‚ÑÑ‚Ð²Ð¾", "ÑÐ¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ñ", "Ð½Ð°Ð»Ð¾Ð³Ð¾Ð²Ð¾Ðµ Ð¿Ñ€Ð°Ð²Ð¾", "Ð½Ð´Ñ„Ð»"],
                "weight": 0.8
            },
            "Data": {
                "keywords": ["Ð´Ð°Ð½Ð½Ñ‹Ðµ", "Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÐ°", "data", "analytics", "bi", "sql", "ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°", 
                           "data engineering", "Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ"],
                "weight": 0.7
            },
            "Architecture": {
                "keywords": ["Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°", "ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹", "Ð¼Ð¸ÐºÑ€Ð¾ÑÐµÑ€Ð²Ð¸ÑÑ‹", "ddd", "Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸", 
                           "Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½Ð½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹", "ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð·"],
                "weight": 0.7
            },
            "Security": {
                "keywords": ["Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ", "security", "ÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð³Ñ€Ð°Ñ„Ð¸Ñ", "iam", "Ð°Ð¿psec", 
                           "Ð½Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ñ‹", "compliance"],
                "weight": 0.7
            }
        }
        
        best_category = "Other"
        best_score = 0.0
        
        for category, data in category_keywords.items():
            score = 0
            for keyword in data["keywords"]:
                if keyword in text_lower:
                    score += 1
            
            # Normalize score and apply weight
            if score > 0:
                normalized_score = min(score / len(data["keywords"]) * data["weight"], 0.9)
                if normalized_score > best_score:
                    best_score = normalized_score
                    best_category = category
        
        # Ensure minimum confidence of 0.25 if keywords found, 0.15 otherwise
        confidence = max(best_score, 0.25 if best_score > 0 else 0.15)
        
        return best_category, confidence

    def is_available(self) -> bool:
        """Check if OpenAI API key is available"""
        return bool(self.client and os.getenv("OPENAI_API_KEY"))